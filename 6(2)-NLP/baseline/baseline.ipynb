{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KorNLI - Natural Language Inference Baseline\n",
    "\n",
    "**Self-contained Colab notebook** - 모든 코드가 노트북에 포함되어 있어 Colab에서 바로 실행 가능합니다.\n",
    "\n",
    "## 사용법\n",
    "1. **런타임 설정**: 런타임 → 런타임 유형 변경 → GPU 선택\n",
    "2. **Setup 실행**: Cell 1 실행 (패키지 설치 및 GPU 확인)\n",
    "3. **데이터 로드**: Cell 3 또는 Cell 4 중 **하나만** 선택하여 실행\n",
    "4. **경로 설정**: Cell 5 실행\n",
    "5. **코드 정의**: Cell 6~11 순서대로 실행\n",
    "6. **학습 및 평가**: Cell 12~14 순서대로 실행\n",
    "\n",
    "## 예상 학습 시간\n",
    "- **T4 GPU 기준**: ~5분 (1 epoch, default 설정)\n",
    "- **CPU**: ~30분 이상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 1: Setup - 패키지 설치 및 GPU 확인\n",
    "# =============================================================================\n",
    "!pip install -q torch transformers pandas scikit-learn tqdm sentencepiece\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드\n",
    "\n",
    "아래 **Option A** 또는 **Option B** 중 **하나만** 선택하여 실행하세요.\n",
    "\n",
    "- **Option A**: 로컬 파일 업로드 (파일을 직접 업로드)\n",
    "- **Option B**: Google Drive 연결 (Drive에 데이터가 있는 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Data Option A - 파일 업로드 (선택 실행)\n",
    "# =============================================================================\n",
    "# 이 셀은 파일 업로드 방식을 원할 때만 실행하세요.\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "print(\"train.tsv, val.tsv, test_unlabeled.tsv 파일을 업로드하세요\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f\"data/{filename}\")\n",
    "    print(f\"  -> data/{filename} 저장 완료\")\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "print(f\"\\nDATA_DIR = '{DATA_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Data Option B - Google Drive (선택 실행)\n",
    "# =============================================================================\n",
    "# 이 셀은 Google Drive 방식을 원할 때만 실행하세요.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 아래 경로를 본인의 Drive 폴더로 수정하세요\n",
    "DATA_DIR = \"/content/drive/MyDrive/kor-nlu-datasets/data\"\n",
    "print(f\"DATA_DIR = '{DATA_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: 경로 설정 (필수 실행)\n",
    "# =============================================================================\n",
    "# DATA_DIR은 위 Option A 또는 B에서 설정되어야 합니다.\n",
    "\n",
    "OUTPUT_DIR = \".\"\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "\n",
    "# 설정된 경로 확인\n",
    "import os\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# 데이터 파일 확인\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"\\nFiles in {DATA_DIR}:\")\n",
    "    for f in os.listdir(DATA_DIR):\n",
    "        print(f\"  - {f}\")\n",
    "else:\n",
    "    print(f\"\\n[WARNING] {DATA_DIR} 디렉토리가 존재하지 않습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Config 정의\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# Label Mapping\n",
    "LABEL2ID = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "NUM_LABELS = len(LABEL2ID)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Configuration Presets\n",
    "CONFIGS = {\n",
    "    \"default\": {\n",
    "        \"model_name\": \"monologg/distilkobert\",\n",
    "        \"max_length\": 64,\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"epochs\": 1,\n",
    "        \"warmup_ratio\": 0.0,\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"classifier_dropout\": None,\n",
    "        \"early_stopping_patience\": None,\n",
    "        \"preprocess\": False,\n",
    "        \"train_file\": \"train.tsv\",\n",
    "        \"dev_file\": \"val.tsv\",\n",
    "        \"test_file\": \"test_unlabeled.tsv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_config(preset: str = \"default\", **overrides) -> dict:\n",
    "    \"\"\"설정 프리셋 로드 + override 적용\"\"\"\n",
    "    if preset not in CONFIGS:\n",
    "        raise ValueError(f\"Unknown preset: {preset}. Available: {list(CONFIGS.keys())}\")\n",
    "    config = CONFIGS[preset].copy()\n",
    "    config.update(overrides)\n",
    "    return config\n",
    "\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Labels: {LABEL2ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 7: Preprocessing 정의\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"기본 텍스트 전처리 (Unicode NFC 정규화 + 공백 정규화)\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "print(\"Preprocessing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Dataset 정의\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def load_data(file_path: str, preprocess: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"TSV 데이터 파일 로드\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", quoting=3, on_bad_lines=\"skip\")\n",
    "    # 결측값 제거\n",
    "    df = df.dropna(subset=[\"sentence1\", \"sentence2\"])\n",
    "    if preprocess:\n",
    "        df[\"sentence1\"] = df[\"sentence1\"].apply(preprocess_text)\n",
    "        df[\"sentence2\"] = df[\"sentence2\"].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    \"\"\"NLI 태스크용 PyTorch Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizer, max_length: int, label2id: dict = None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        row = self.df.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            row[\"sentence1\"],\n",
    "            row[\"sentence2\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        if self.label2id and \"gold_label\" in row:\n",
    "            item[\"labels\"] = torch.tensor(self.label2id[row[\"gold_label\"]])\n",
    "        return item\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_df: pd.DataFrame,\n",
    "    dev_df: pd.DataFrame,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_length: int,\n",
    "    batch_size: int,\n",
    "    test_df: pd.DataFrame = None,\n",
    ") -> tuple:\n",
    "    \"\"\"DataLoader 일괄 생성\"\"\"\n",
    "    train_dataset = NLIDataset(train_df, tokenizer, max_length, LABEL2ID)\n",
    "    dev_dataset = NLIDataset(dev_df, tokenizer, max_length, LABEL2ID)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    if test_df is not None:\n",
    "        test_dataset = NLIDataset(test_df, tokenizer, max_length, LABEL2ID)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        return train_loader, dev_loader, test_loader\n",
    "\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "\n",
    "print(\"Dataset classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: Model 정의\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "def load_tokenizer(model_name: str) -> PreTrainedTokenizer:\n",
    "    \"\"\"토크나이저 로드\"\"\"\n",
    "    return AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def load_model(model_name: str, classifier_dropout: float = None, device: torch.device = None) -> PreTrainedModel:\n",
    "    \"\"\"분류 모델 로드\"\"\"\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name, num_labels=NUM_LABELS, trust_remote_code=True)\n",
    "\n",
    "    if classifier_dropout is not None:\n",
    "        if hasattr(model_config, \"classifier_dropout\"):\n",
    "            model_config.classifier_dropout = classifier_dropout\n",
    "        elif hasattr(model_config, \"hidden_dropout_prob\"):\n",
    "            model_config.hidden_dropout_prob = classifier_dropout\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=model_config, trust_remote_code=True)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_optimizer(model: PreTrainedModel, learning_rate: float, weight_decay: float = 0.0) -> torch.optim.Optimizer:\n",
    "    \"\"\"AdamW 옵티마이저 생성\"\"\"\n",
    "    return torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "def create_scheduler(optimizer: torch.optim.Optimizer, num_training_steps: int, warmup_ratio: float = 0.0):\n",
    "    \"\"\"Linear warmup 스케줄러 생성 (warmup_ratio > 0일 때만)\"\"\"\n",
    "    if warmup_ratio <= 0:\n",
    "        return None\n",
    "    warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "    return get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "def save_model(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, save_dir: str) -> None:\n",
    "    \"\"\"모델과 토크나이저 저장\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"{save_dir}/best_model.pt\")\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "\n",
    "def load_checkpoint(model: PreTrainedModel, checkpoint_path: str, device: torch.device = None) -> PreTrainedModel:\n",
    "    \"\"\"체크포인트에서 모델 가중치 로드\"\"\"\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Model functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Training 정의\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "def train_epoch(model: PreTrainedModel, dataloader: DataLoader, optimizer, scheduler=None, device=None) -> float:\n",
    "    \"\"\"한 에폭 학습 - 평균 손실 반환\"\"\"\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: PreTrainedModel,\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader: DataLoader,\n",
    "    optimizer,\n",
    "    epochs: int,\n",
    "    scheduler=None,\n",
    "    early_stopping_patience: int = None,\n",
    "    checkpoint_dir: str = None,\n",
    "    device=None,\n",
    ") -> dict:\n",
    "    \"\"\"전체 학습 - 결과 dict 반환\"\"\"\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        dev_accuracy, _, _ = evaluate(model, dev_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Dev Accuracy: {dev_accuracy:.4f}\")\n",
    "\n",
    "        if dev_accuracy > best_accuracy:\n",
    "            best_accuracy = dev_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "\n",
    "            if checkpoint_dir:\n",
    "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if early_stopping_patience and patience_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    return {\"best_accuracy\": best_accuracy, \"best_epoch\": best_epoch}\n",
    "\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Evaluation & Prediction 정의\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "\n",
    "def evaluate(model: PreTrainedModel, dataloader: DataLoader, device: torch.device = None) -> tuple:\n",
    "    \"\"\"모델 평가 - (accuracy, predictions, true_labels) 반환\"\"\"\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy, predictions, true_labels\n",
    "\n",
    "\n",
    "def get_classification_report(true_labels: list, predictions: list) -> str:\n",
    "    \"\"\"Classification report 문자열 반환\"\"\"\n",
    "    return classification_report(true_labels, predictions, target_names=list(LABEL2ID.keys()))\n",
    "\n",
    "\n",
    "# --- Prediction ---\n",
    "\n",
    "def predict(model: PreTrainedModel, dataloader: DataLoader, device: torch.device = None) -> list:\n",
    "    \"\"\"모델 예측 - 예측 ID 리스트 반환\"\"\"\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def create_submission(predictions: list, id2label: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"submission DataFrame 생성\"\"\"\n",
    "    if id2label is None:\n",
    "        id2label = ID2LABEL\n",
    "    pred_labels = [id2label[p] for p in predictions]\n",
    "    return pd.DataFrame({\"id\": range(len(pred_labels)), \"label\": pred_labels})\n",
    "\n",
    "\n",
    "def save_submission(submission_df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"submission CSV 저장\"\"\"\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "print(\"Evaluation & Prediction functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: 실행 - Config, Data, Model, DataLoader\n",
    "# =============================================================================\n",
    "\n",
    "# Config 로드\n",
    "PRESET = \"default\"\n",
    "config = get_config(PRESET)\n",
    "print(f\"Preset: {PRESET}\")\n",
    "print(f\"Config: {config}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Data 로드\n",
    "train_df = load_data(f\"{DATA_DIR}/{config['train_file']}\", preprocess=config[\"preprocess\"])\n",
    "dev_df = load_data(f\"{DATA_DIR}/{config['dev_file']}\", preprocess=config[\"preprocess\"])\n",
    "test_df = load_data(f\"{DATA_DIR}/{config['test_file']}\", preprocess=config[\"preprocess\"])\n",
    "print(f\"\\nData loaded - Train: {len(train_df)}, Dev: {len(dev_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Tokenizer 로드\n",
    "tokenizer = load_tokenizer(config[\"model_name\"])\n",
    "print(f\"Tokenizer loaded: {config['model_name']}\")\n",
    "\n",
    "# Model 로드\n",
    "model = load_model(config[\"model_name\"], classifier_dropout=config[\"classifier_dropout\"])\n",
    "print(f\"Model loaded: {config['model_name']}\")\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader, dev_loader, test_loader = create_dataloaders(\n",
    "    train_df, dev_df, tokenizer, config[\"max_length\"], config[\"batch_size\"], test_df\n",
    ")\n",
    "print(f\"\\nDataLoaders created - Train batches: {len(train_loader)}, Dev batches: {len(dev_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 13: 실행 - Training & Save\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# Optimizer & Scheduler 생성\n",
    "optimizer = create_optimizer(model, config[\"learning_rate\"], config[\"weight_decay\"])\n",
    "scheduler = create_scheduler(optimizer, len(train_loader) * config[\"epochs\"], config[\"warmup_ratio\"])\n",
    "\n",
    "# Training\n",
    "print(\"=\" * 50)\n",
    "print(\"Training started...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = train(\n",
    "    model, train_loader, dev_loader, optimizer, config[\"epochs\"],\n",
    "    scheduler=scheduler,\n",
    "    early_stopping_patience=config[\"early_stopping_patience\"],\n",
    "    checkpoint_dir=CHECKPOINT_DIR\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training completed! Best Accuracy: {result['best_accuracy']:.4f} (Epoch {result['best_epoch']})\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Tokenizer & Config 저장\n",
    "try:\n",
    "    tokenizer.save_pretrained(CHECKPOINT_DIR)\n",
    "except TypeError:\n",
    "    pass  # KoBertTokenizer doesn't support filename_prefix argument\n",
    "\n",
    "with open(f\"{CHECKPOINT_DIR}/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Tokenizer & Config saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 14: 실행 - Evaluate, Predict & Submission 저장\n",
    "# =============================================================================\n",
    "\n",
    "# Best 모델 로드\n",
    "model = load_checkpoint(model, f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "print(\"Best model loaded from checkpoint.\")\n",
    "\n",
    "# Dev set 평가\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Evaluation on Dev set\")\n",
    "print(\"=\" * 50)\n",
    "accuracy, preds, labels = evaluate(model, dev_loader)\n",
    "print(f\"\\nDev Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(get_classification_report(labels, preds))\n",
    "\n",
    "# Test set 예측\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Prediction on Test set\")\n",
    "print(\"=\" * 50)\n",
    "test_preds = predict(model, test_loader)\n",
    "\n",
    "# Submission 저장\n",
    "submission_df = create_submission(test_preds)\n",
    "submission_path = f\"{OUTPUT_DIR}/submission.csv\"\n",
    "save_submission(submission_df, submission_path)\n",
    "print(f\"\\nSubmission saved: {submission_path}\")\n",
    "\n",
    "# 결과 미리보기\n",
    "print(\"\\nSubmission preview:\")\n",
    "display(submission_df.head(10))\n",
    "\n",
    "# Colab에서 다운로드\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(submission_path)\n",
    "    print(f\"\\n[Download] {submission_path} 다운로드가 시작됩니다.\")\n",
    "except ImportError:\n",
    "    print(f\"\\n[Info] Colab 환경이 아닙니다. {submission_path}에서 파일을 확인하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
